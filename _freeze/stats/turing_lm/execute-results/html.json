{
  "hash": "96ba2775670cbe3b7558644677e2671e",
  "result": {
    "markdown": "---\ntitle: Bayesian Linear Regression\ndescription: |\n  Illustrated using Julia\ndate: '2022-12-14'\nformat:\n  html:\n    code-fold: false\n---\n\nAn example of using Bayesian methods (via Julia's `Turing.jl`) to estimate a linear regression\n\nLoad Packages\n\n::: {.cell execution_count=1}\n``` {.julia .cell-code}\nusing Turing\nusing Random\nusing Distributions\nusing LinearAlgebra\nusing Plots\nusing StatsPlots\n```\n:::\n\n\nGenerate some fake data\n\n::: {.cell execution_count=2}\n``` {.julia .cell-code}\nRandom.seed!(0408)\n\nn = 1000\n\nùêó = randn(n, 3)\n\nŒ≤ = [1., 2., 3.]\n\nf(x) = .5 .+ x*Œ≤\n\nœµ = rand(Normal(0, .2), n)\n\ny = f(ùêó) + œµ;\n```\n:::\n\n\nDefine a Model\n\n::: {.cell execution_count=3}\n``` {.julia .cell-code}\n@model function linear_regression(x, y)\n    #housekeeping\n    n_feat = size(x, 2)\n    \n    #priors\n    Œ± ~ Normal(0, 2)\n    œÉ ~ Exponential(1)\n    b ~ MvNormal(zeros(n_feat), 5 * I)\n\n    #likelihood\n    for i ‚àà eachindex(y)\n        y[i] ~ Normal(Œ± + x[i,:]' * b, œÉ)\n    end\nend\n\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```\nlinear_regression (generic function with 2 methods)\n```\n:::\n:::\n\n\nCompute Posterior\n\n::: {.cell execution_count=4}\n``` {.julia .cell-code}\nmodel = linear_regression(ùêó, y)\n\nchn = sample(model, NUTS(), MCMCThreads(), 1_000, 2);\n```\n:::\n\n\nPlot Parameter Posteriors\n\n::: {.cell execution_count=5}\n``` {.julia .cell-code}\nplot(chn)\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n![](turing_lm_files/figure-html/cell-6-output-1.svg){}\n:::\n:::\n\n\nPredict Values of Y\n\n::: {.cell execution_count=6}\n``` {.julia .cell-code}\npred_mod = linear_regression(\n    ùêó, \n    Vector{Union{Missing, Float64}}(undef, length(y))\n)\n\npreds = predict(pred_mod, chn);\n\n#to get summary statistics\nsummarize(preds)\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre>Summary Statistics\n <span class=\"ansi-bold\"> parameters </span> <span class=\"ansi-bold\">    mean </span> <span class=\"ansi-bold\">     std </span> <span class=\"ansi-bold\"> naive_se </span> <span class=\"ansi-bold\">    mcse </span> <span class=\"ansi-bold\">       ess </span> <span class=\"ansi-bold\">    rhat </span>\n <span class=\"ansi-bright-black-fg\">     Symbol </span> <span class=\"ansi-bright-black-fg\"> Float64 </span> <span class=\"ansi-bright-black-fg\"> Float64 </span> <span class=\"ansi-bright-black-fg\">  Float64 </span> <span class=\"ansi-bright-black-fg\"> Float64 </span> <span class=\"ansi-bright-black-fg\">   Float64 </span> <span class=\"ansi-bright-black-fg\"> Float64 </span>\n        y[1]    5.6876    0.2120     0.0047    0.0040   2131.4185    1.0007\n        y[2]   -1.0132    0.2088     0.0047    0.0050   2045.6712    1.0004\n        y[3]   -3.1168    0.2059     0.0046    0.0039   2026.1454    0.9993\n        y[4]    4.5125    0.2042     0.0046    0.0047   1994.3306    0.9997\n        y[5]    2.6687    0.2041     0.0046    0.0036   2064.6256    0.9991\n        y[6]   -1.2255    0.2080     0.0047    0.0049   1836.3394    0.9993\n        y[7]    2.3067    0.2054     0.0046    0.0039   2075.1743    1.0003\n        y[8]    1.1287    0.2042     0.0046    0.0039   1929.8026    0.9999\n        y[9]    0.4400    0.2061     0.0046    0.0041   2109.2564    0.9996\n       y[10]    6.7965    0.2053     0.0046    0.0047   1746.0770    1.0008\n       y[11]   -3.7930    0.2034     0.0045    0.0048   1820.9121    0.9993\n       y[12]    0.4814    0.2053     0.0046    0.0046   1868.2305    0.9996\n       y[13]    2.4322    0.2068     0.0046    0.0045   1941.8888    0.9993\n       y[14]    0.3437    0.2071     0.0046    0.0048   1761.8573    1.0017\n       y[15]   -0.7060    0.2091     0.0047    0.0046   2053.6154    1.0004\n       y[16]    0.3729    0.2086     0.0047    0.0048   1927.2731    0.9993\n       y[17]    1.8546    0.2099     0.0047    0.0047   1810.0899    0.9996\n       y[18]    2.2985    0.2073     0.0046    0.0045   1978.9502    0.9994\n       y[19]   -0.8137    0.2073     0.0046    0.0047   2090.7345    0.9995\n       y[20]    4.8043    0.2024     0.0045    0.0048   1878.8739    0.9993\n       y[21]    3.7083    0.2100     0.0047    0.0052   1885.7547    0.9996\n       y[22]    0.2769    0.2095     0.0047    0.0036   1944.5012    0.9996\n       y[23]   -3.4487    0.2122     0.0047    0.0054   2100.7819    1.0001\n      ‚ãÆ           ‚ãÆ         ‚ãÆ         ‚ãÆ          ‚ãÆ          ‚ãÆ          ‚ãÆ\n<span class=\"ansi-cyan-fg\">                                                             977 rows omitted</span>\n</pre>\n```\n:::\n\n:::\n:::\n\n\nPlot posterior distribution(s) of the predictions for the first observation:\n\n::: {.cell execution_count=7}\n``` {.julia .cell-code}\ny_1 = getindex(preds, \"y[1]\")\n\ndensity(y_1.data)\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n![](turing_lm_files/figure-html/cell-8-output-1.svg){}\n:::\n:::\n\n\nAnd to get mean predicted values for each observation of y:\n\n::: {.cell execution_count=8}\n``` {.julia .cell-code}\nmean_preds = summarize(preds)[:, 2]\n```\n\n::: {.cell-output .cell-output-display execution_count=9}\n```\n1000-element Vector{Float64}:\n  5.687584896870903\n -1.0131932466821032\n -3.116780621735853\n  4.512509987465371\n  2.6687246425133946\n -1.2254837248151988\n  2.306673192525782\n  1.1286618139435005\n  0.44000862835633064\n  6.79652703562579\n -3.79298730457439\n  0.48144060256761656\n  2.4321882922004816\n  ‚ãÆ\n -3.662775430736099\n  0.7859815782511625\n -3.5565609003804592\n -0.9696845244200145\n  0.82043625177212\n  4.348165410409958\n  1.9125271244452688\n  1.072526007780797\n -7.133862685203614\n -6.206271121676054\n -7.631370737446044\n  0.7668219857089003\n```\n:::\n:::\n\n\n",
    "supporting": [
      "turing_lm_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}
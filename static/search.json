[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "Hi, friend. I‚Äôm Eric. If you‚Äôre interested in learning more about me, you can check out my personal site, where I occassionally blog and post stuff. You can also find me on Github"
  },
  {
    "objectID": "Go/go_index.html",
    "href": "Go/go_index.html",
    "title": "Go Placeholder",
    "section": "",
    "text": "Some text"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "leakybrain",
    "section": "",
    "text": "Welcome to leakybrain.\nThis is a site I created to drop random snippets of code and take notes related to programming, statistics, and other related topics. It was inspired by Vicki Boykis‚Äôs BoringML project."
  },
  {
    "objectID": "Julia/julia_pkg_init.html",
    "href": "Julia/julia_pkg_init.html",
    "title": "Julia Package Initialization",
    "section": "",
    "text": "Creating a new Julia package is easiest if you create whatever.jl script to help establish the template, e.g.\ncd some/path\ntouch whatever.jl\nThen open this with VSCode. Once in VSCode, open the terminal and install PkgTemplates.jl if it‚Äôs not already installed:\n]\nadd PkgTemplates\nThen create a template with the options you want in your new package. The way I currently do this is:\nusing PkgTemplates\n\nt = Template(;\n    user = \"ekholme\", #or your git user.name\n    license = \"MIT\",\n    authors = [\"Eric Ekholm\"],\n    julia_version = v\"1.8\", #or w/e version you want\n    plugins = [\n        GitHubActions(),\n        Codecov(),\n        GitHubPages(),\n        TravisCI()\n    ]\n)\n\nt(\"MyPkgName\")\nAnd then running this will create a new package (and set up all of your Git stuff if it‚Äôs already configured)"
  },
  {
    "objectID": "R/pre_alloc_loops.html",
    "href": "R/pre_alloc_loops.html",
    "title": "Pre-Allocating to Improve For Loops",
    "section": "",
    "text": "How to pre-allocate vectors used in your for loops in R to make your code run faster and perform fewer allocations.\nFirst, set n to be the size of the vector we want to work with\n\nn <- 1e6\n\nThen set up a function that doesn‚Äôt pre-allocate a list\n\nno_alloc <- function(n) {\n\n    x <- list()\n\n    for (i in seq_len(n)) {\n        x[[i]] <- i\n    }\n\n    x\n}\n\nThen set up a function that does pre-allocate a list\n\npre_alloc <- function(n) {\n    \n    x <- vector(mode = \"list\", length = n)\n\n    for (i in seq_len(n)) {\n        x[[i]] <- i\n    }\n    \n    x\n}\n\nAnd compare the benchmarks for these functions\n\nlibrary(bench)\n\nWarning: package 'bench' was built under R version 4.2.2\n\nres <- bench::mark(\n    no_alloc(n),\n    pre_alloc(n)\n)\n\nWarning: Some expressions had a GC in every iteration; so filtering is disabled.\n\nres[c(\"expression\", \"min\", \"median\", \"itr/sec\", \"n_gc\")]\n\n# A tibble: 2 √ó 4\n  expression        min   median `itr/sec`\n  <bch:expr>   <bch:tm> <bch:tm>     <dbl>\n1 no_alloc(n)   632.2ms  632.2ms      1.58\n2 pre_alloc(n)   56.5ms   74.4ms     12.7 \n\n\nWe can see that pre-allocating is the way to go!"
  },
  {
    "objectID": "Stats/mle_lm.html",
    "href": "Stats/mle_lm.html",
    "title": "Maximum Likelihood Estimation - Linear Regression",
    "section": "",
    "text": "An example of estimating regression coefficients in a linear model via maximum likelihood, using Julia.\n\nusing Distributions\nusing Random\nusing Optim\nusing GLM\n\nGenerate some fake data\n\nRandom.seed!(0408)\n\n#x data\nùêó = hcat(ones(1000), randn(1000, 3))\n\n#ground truth betas\nùö© = [.5, 1, 2, 3]\n\n#multiply data by betas\nf‚ÇÅ(X) = X*ùö©\n\n#make some error\nœµ = rand(Normal(0, .5), size(ùêó)[1])\n\n#generate y\ny = f‚ÇÅ(ùêó) + œµ;\n\nDefine a function to optimize\n\nfunction mle_lm(x, y, params)\n    b = params[begin:end-1]\n    œÉ = params[end]\n\n    yÃÇ = x*b\n\n    residuals = y .- yÃÇ\n\n    ll = -loglikelihood(Normal(0, œÉ), residuals)\n\n    return ll\nend\n\nmle_lm (generic function with 1 method)\n\n\nRun the optimization\n\nstart_params = [0.2, .5, 1, 1, 1]\n\nres = optimize(params -> mle_lm(ùêó, y, params), start_params)\n\nOptim.minimizer(res)\n\n5-element Vector{Float64}:\n 0.5220526008168841\n 0.9925044101015756\n 1.9951661029827337\n 2.9979617225853903\n 0.5170359122024899\n\n\nCheck against ‚Äòbase‚Äô Julia solution\n\nùêó \\ y\n\n4-element Vector{Float64}:\n 0.5220524130050495\n 0.9925035386795747\n 1.9951668631756507\n 2.9979619869409357"
  },
  {
    "objectID": "Stats/turing_lm.html",
    "href": "Stats/turing_lm.html",
    "title": "Bayesian Linear Regression",
    "section": "",
    "text": "An example of using Bayesian methods (via Julia‚Äôs Turing.jl) to estimate a linear regression\nLoad Packages\n\nusing Turing\nusing Random\nusing Distributions\nusing LinearAlgebra\nusing Plots\nusing StatsPlots\n\nGenerate some fake data\n\nRandom.seed!(0408)\n\nn = 1000\n\nùêó = randn(n, 3)\n\nŒ≤ = [1., 2., 3.]\n\nf(x) = .5 .+ x*Œ≤\n\nœµ = rand(Normal(0, .2), n)\n\ny = f(ùêó) + œµ;\n\nDefine a Model\n\n@model function linear_regression(x, y)\n    #housekeeping\n    n_feat = size(x, 2)\n    \n    #priors\n    Œ± ~ Normal(0, 2)\n    œÉ ~ Exponential(1)\n    b ~ MvNormal(zeros(n_feat), 5 * I)\n\n    #likelihood\n    for i ‚àà eachindex(y)\n        y[i] ~ Normal(Œ± + x[i,:]' * b, œÉ)\n    end\nend\n\nlinear_regression (generic function with 2 methods)\n\n\nCompute Posterior\n\nmodel = linear_regression(ùêó, y)\n\nchn = sample(model, NUTS(), MCMCThreads(), 1_000, 2);\n\nPlot Parameter Posteriors\n\nplot(chn)\n\n\n\n\nPredict Values of Y\n\npred_mod = linear_regression(\n    ùêó, \n    Vector{Union{Missing, Float64}}(undef, length(y))\n)\n\npreds = predict(pred_mod, chn);\n\n#to get summary statistics\nsummarize(preds)\n\n\nSummary Statistics\n  parameters      mean       std   naive_se      mcse         ess      rhat \n      Symbol   Float64   Float64    Float64   Float64     Float64   Float64 \n        y[1]    5.6876    0.2120     0.0047    0.0040   2131.4185    1.0007\n        y[2]   -1.0132    0.2088     0.0047    0.0050   2045.6712    1.0004\n        y[3]   -3.1168    0.2059     0.0046    0.0039   2026.1454    0.9993\n        y[4]    4.5125    0.2042     0.0046    0.0047   1994.3306    0.9997\n        y[5]    2.6687    0.2041     0.0046    0.0036   2064.6256    0.9991\n        y[6]   -1.2255    0.2080     0.0047    0.0049   1836.3394    0.9993\n        y[7]    2.3067    0.2054     0.0046    0.0039   2075.1743    1.0003\n        y[8]    1.1287    0.2042     0.0046    0.0039   1929.8026    0.9999\n        y[9]    0.4400    0.2061     0.0046    0.0041   2109.2564    0.9996\n       y[10]    6.7965    0.2053     0.0046    0.0047   1746.0770    1.0008\n       y[11]   -3.7930    0.2034     0.0045    0.0048   1820.9121    0.9993\n       y[12]    0.4814    0.2053     0.0046    0.0046   1868.2305    0.9996\n       y[13]    2.4322    0.2068     0.0046    0.0045   1941.8888    0.9993\n       y[14]    0.3437    0.2071     0.0046    0.0048   1761.8573    1.0017\n       y[15]   -0.7060    0.2091     0.0047    0.0046   2053.6154    1.0004\n       y[16]    0.3729    0.2086     0.0047    0.0048   1927.2731    0.9993\n       y[17]    1.8546    0.2099     0.0047    0.0047   1810.0899    0.9996\n       y[18]    2.2985    0.2073     0.0046    0.0045   1978.9502    0.9994\n       y[19]   -0.8137    0.2073     0.0046    0.0047   2090.7345    0.9995\n       y[20]    4.8043    0.2024     0.0045    0.0048   1878.8739    0.9993\n       y[21]    3.7083    0.2100     0.0047    0.0052   1885.7547    0.9996\n       y[22]    0.2769    0.2095     0.0047    0.0036   1944.5012    0.9996\n       y[23]   -3.4487    0.2122     0.0047    0.0054   2100.7819    1.0001\n      ‚ãÆ           ‚ãÆ         ‚ãÆ         ‚ãÆ          ‚ãÆ          ‚ãÆ          ‚ãÆ\n                                                             977 rows omitted\n\n\n\n\nPlot posterior distribution(s) of the predictions for the first observation:\n\ny_1 = getindex(preds, \"y[1]\")\n\ndensity(y_1.data)\n\n\n\n\nAnd to get mean predicted values for each observation of y:\n\nmean_preds = summarize(preds)[:, 2]\n\n1000-element Vector{Float64}:\n  5.687584896870903\n -1.0131932466821032\n -3.116780621735853\n  4.512509987465371\n  2.6687246425133946\n -1.2254837248151988\n  2.306673192525782\n  1.1286618139435005\n  0.44000862835633064\n  6.79652703562579\n -3.79298730457439\n  0.48144060256761656\n  2.4321882922004816\n  ‚ãÆ\n -3.662775430736099\n  0.7859815782511625\n -3.5565609003804592\n -0.9696845244200145\n  0.82043625177212\n  4.348165410409958\n  1.9125271244452688\n  1.072526007780797\n -7.133862685203614\n -6.206271121676054\n -7.631370737446044\n  0.7668219857089003"
  }
]
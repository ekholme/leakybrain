---
title: "Gradient Descent"
description: |
    Illustrated using Julia
date: "2023-04-21"
format:
    html:
        code-fold: false
jupyter: julia-1.8
---

An example of estimating linear regression beta coefficients via gradient descent, using Julia.

```{julia}
using Random
using GLM
using ForwardDiff
using Statistics
```

# Generate Data

First, we generate some fake data

```{julia}
Random.seed!(0408)

#x data
ğ— = hcat(ones(1000), randn(1000, 3))

#ground truth betas
ğš© = [.5, 1, 2, 3]

#multiply data by betas
fâ‚(X) = X*ğš©

#make some error
Ïµ = rand(Normal(0, .5), size(ğ—)[1])

#generate y
y = fâ‚(ğ—) + Ïµ;
``` 

# Define a Loss Function

Mean squared error is the most straightforward

```{julia}
function mse_loss(X, y, b)
    yÌ‚ = X*b

    l = mean((y .- yÌ‚).^2)

    return l
end
```

# Define a training function

This implements the gradient descent algorithm:

- initialize some random beta values
- initialize error as some very large number (the init value doesn't really matter as long as it's greater than the function's `tol` parameter)
- initialize the number of iterations (`iter`) at 0
- define a function `d()` to get the gradient of the loss function at a given set of betas
- define a loop that updates the beta values by the learning rate * the gradients until convergence

```{julia}
function grad_descent(X, y; lr = .01, tol = .01, max_iter = 1_000, noisy = false)
   #randomly initialize betas
   Î² = rand(size(X)[2])
   
    #init error to something large
    err = 1e10

    #initialize iterations at 0
    iter = 0

    #define a function to get the gradient of the loss function at a given set of betas
    d(b) = ForwardDiff.gradient(params -> mse_loss(X, y, params), b)

    while err > tol && iter < max_iter
        Î² -= lr*d(Î²)
        err = mse_loss(X, y, Î²)
        if (noisy == true)
            println("Iteration $(iter): current error is $(err)")
        end
        iter += 1
    end
    return Î²
end
```

# Estimate Î²s

To estimate the betas, we just run the function

```{julia}
b = grad_descent(ğ—, y)
```

# Check Solution Against Base Julia Solver

```{julia}
ğ— \ y .â‰ˆ b
```

huzzah!
{
  "hash": "321deac172b707368d2b3c8bc7c1605a",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Fully Connected Network in PyTorch\"\ndescription: |\n  Implementing a fully connected neural network in PyTorch\ndate: 2024-08-16\nformat:\n  html:\n    code-fold: false\nexecute:\n  freeze: auto\njupyter: python3\n---\n\n\nThis page contains my notes on how to implement a fully-connected neural network (FCN) in PyTorch. It's meant to be a bare-bones, basic implementation of an FCN.\n\n## What is an FCN?\n\nA fully connected network (FCN) is an architecture with a bunch of densely-connected layers stacked on top of one another. In this architecture, each node in layer *i* is connected to each node of layer *i+1*, etc, with the output of each layer being passed through an activation function before being sent to the next layer. Here's what this looks like:\n\n![](https://miro.medium.com/v2/resize:fit:720/1*VHOUViL8dHGfvxCsswPv-Q.png)\n\nIn PyTorch parlance, these are linear layers, and they can be specified via `nn.Linear(in_size, out_size)`. Other frameworks might call them other things (e.g. Flux.jl calls them [Dense layers](https://fluxml.ai/Flux.jl/previews/PR1613/models/layers/)).\n\n### Strengths and Weaknesses\n\nFCNs are flexible models that can do well at all sorts of \"typical\" machine-learning tasks (i.e. classification and regression), and can work with various types of inputs (images, text, etc.). They're basically the linear regression of the deep learning world (in that they're foundational models, not in the sense that they're linear...because they aren't).\n\nSince FCNs connect all nodes in layer *i* to all nodes in layer *i+1*, they can be computationally expensive when used to model large datasets (those with lots of predictors) or when the models themselves have lots of parameters. Another issue is that they don't have any mechanisms to capture dependencies in the data, so in cases where the inputs have dependencies (image data -> spatial dependenices, time-series data -> temporal dependencies, clustered data -> group dependencies), FCNs may not be the best choice.\n\n## Example Model in PyTorch\n\nThe code below has a basic implementation of a fully-connected network in PyTorch. I'm using fake data and an arbitrary model architecture, so it's not supposed to be a great model. It's more intended to demonstrate a general workflow.\n\n### Import Libraries\n\nThe main library I need here is `torch`. Then I'm also loading the `nn` module to create the FCN as well as some utility functions to work with the data.\n\n::: {#235c66f9 .cell execution_count=1}\n``` {.python .cell-code}\n#import libraries\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, TensorDataset, random_split\n```\n:::\n\n\n### Generate Fake Data\n\nIn this step, I'm generating some fake data:\n\n- a design matrix, `X`,\n- a set of ground-truth betas\n- the result of $X * \\beta$, `y_true`\n- `y_true` with some noise added to it, `y_noisy`\n\n::: {#712d5461 .cell execution_count=2}\n``` {.python .cell-code}\n# generate some true data\nn = 10000\nm = 10\n\nbeta = torch.randn(m)\nX = torch.randn(n, m)\ny_true = torch.matmul(X, beta)\ny_noisy = y_true + torch.randn(n)\ny_noisy = y_noisy.unsqueeze(1)\n```\n:::\n\n\nIn the last step, I have to call `unsqueeze()` on `y_noisy` to ensure it's formatted as a tensor with the correct number of dimensions.\n\n### Process Data\n\nHere, I'll create a `[Dataset](https://pytorch.org/docs/stable/data.html)` using `X` and `y_noisy`, then I'll do some train/test split stuff and create a `[Dataloader](https://pytorch.org/docs/stable/data.html)` for the train and test sets.\n\nI should probably create a set of notes on datasets and dataloaders in PyTorch, but for now we'll just say that dataloaders are utility tools that help feed data into PyTorch models in batches. These are usually useful when we're working with big data that we can't (or don't want to) process all at once.\n\n::: {#bd8f04ef .cell execution_count=3}\n``` {.python .cell-code}\nbatch_size = 64\n\nds = TensorDataset(X, y_noisy)\n\n#splitting into train and test\ntrn_size = int(.8 * len(X))\ntst_size = len(X) - trn_size\n\ntrn, tst = random_split(ds, [trn_size, tst_size])\n\ntrn_dl = DataLoader(trn, batch_size=batch_size)\ntst_dl = DataLoader(tst, batch_size=batch_size)\n```\n:::\n\n\n### Defining the Model \n\nNow that the data's set up, it's time to define the model. FCN's are pretty straightforward and are composed of alternating `nn.Linear()` and activation function (e.g. `nn.ReLU()`) calls. These can be wrapped in `nn.Sequential()`, which makes it easier to refer to the whole stack of layers as a single module.\n\nI'm also using CUDA if it's available.\n\nThe model definition has 2 parts:\n\n- defining an `__init__()` method;\n- defining a `forward()` method.\n\n`__init__()` defines the model structure/components of the model, and it tells us in the very first line (`class FCN(nn.Module)`) that our class we're creating, `FCN`, is a subclass of (or inherits from) the `nn.Module` class. We then call the nn.Module init function (`super().__init__()`) and define what our model looks like. In this case, it's a fully-connected sequential model. There are 10 inputs into the first layer since there are 10 columns in our `X` matrix. The choice to have 100 output features is arbitrary here, as is the size of the second linear layer (`nn.Linear(100, 100)`). The output size of the final layer is 1 since this is a regression problem, and we want our output to be a single number (in a classification problem, this would be size *k* where *k* is the number of classes in the y variable).\n\nThe `forward()` method defines the order we should call the model components in. In the current case, this is very straightforward, since we've already wrapped all of the individual layers in `nn.Sequential()` and assigned that sequential model to an object called `linear_stack`. So in `forward()`, all we need to do is call the `linear_stack()`. \n\n::: {#ab7a77fa .cell execution_count=4}\n``` {.python .cell-code}\ndevice = (\n    \"cuda\"\n    if torch.cuda.is_available()\n    else \"cpu\"\n)\n\n#define a fully connected model\nclass FCN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear_stack = nn.Sequential(\n            nn.Linear(10, 100),\n            nn.ReLU(),\n            nn.Linear(100, 100),\n            nn.ReLU(),\n            nn.Linear(100, 1)\n        )\n\n    def forward(self, x):\n        ret = self.linear_stack(x)\n        return ret\n    \nmodel=FCN().to(device)\nprint(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFCN(\n  (linear_stack): Sequential(\n    (0): Linear(in_features=10, out_features=100, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=100, out_features=100, bias=True)\n    (3): ReLU()\n    (4): Linear(in_features=100, out_features=1, bias=True)\n  )\n)\n```\n:::\n:::\n\n\nWe could also define the model like this:\n\n```python\nclass FCN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.l1 = nn.Linear(10, 100)\n        self.l2 = nn.Linear(100, 100)\n        self.l3 = nn.Linear(100, 1)\n\n    def forward(self, x):\n        x = self.l1(x)\n        x = F.relu(x)\n        x = self.l2(x)\n        x = F.relu(x)\n        ret = self.l3(x)\n        return ret\n```\n\nBut that doesn't feel quite as good to me.\n\n### Define a Loss Function and Optimizer\n\nThese are fairly straightforward. The loss function is going to be mean squared error (MSE) since it's a regression problem. There are lots of optimizers we could use, but I don't think it actually matters all that much here, so I'll just use stochastic gradient descent (SGD).\n\n::: {#dcf67be1 .cell execution_count=5}\n``` {.python .cell-code}\nloss_fn = nn.MSELoss()\nopt = torch.optim.SGD(model.parameters(), lr=1e-3)\n```\n:::\n\n\n### Define a Train Method\n\nNow we have a model architecture specified, we have a dataloader, we have a loss function, and we have an optimizer. These are the pieces we need to train a model. So we can write a `train()` function that takes these components as arguments. Here's what this function could look like:\n\n::: {#1403d388 .cell execution_count=6}\n``` {.python .cell-code}\ndef train(dataloader, model, loss_fn, optimizer):\n    #just getting the size for printing stuff\n    size = len(dataloader.dataset)\n    #note that model.train() puts the model in 'training mode', which allows for gradient calculation\n    #model.eval() is its contrasting mode\n    model.train()\n    for batch, (X, y) in enumerate(dataloader):\n        X, y = X.to(device), y.to(device)\n\n        #compute error\n        pred = model(X)\n        loss = loss_fn(pred, y)\n\n        #backprop\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n        if batch % 10 == 0:\n            loss, current = loss.item(), (batch + 1) * len(X)\n            print(f\"loss: {loss:>5f} [{current:>5d}/{size:>5d}]\")\n\n```\n:::\n\n\nLet's walk through this:\n\n- We get `size` just to help with printing progress\n- `model.train()` puts the model in \"train\" mode, which lets us calculate the gradient\n- We then iterate over all of the batches in our dataloader...\n- We move `X` and `y` to the GPU (if it's available)\n- Make predictions from the model\n- Calculate the loss (using the specified loss function)\n- Calculate the gradient of the loss function for all model parameters (via `loss.backward()`)\n- Update the model parameters by applying the optimizer's rules (`optimizer.step()`)\n- Zero out the gradients so they can be calculated again (`optimizer.zero_grad()`)\n- Then we do some printing at the end of the loop to show our progress.\n\n### Define the Test Method\n\nUnlike the `train()` function we just defined, `test()` doesn't do parameter optimization -- it simply shows how well the model performs on a holdout (test) set of data. This is why we don't need to include the optimizer as a function argument -- we aren't doing any optimizing.\n\nHere's the code for our `test()` function:\n\n::: {#a2fbd338 .cell execution_count=7}\n``` {.python .cell-code}\ndef test(dataloader, model, loss_fn):\n    size = len(dataloader.dataset)\n    #set model into eval mode\n    model.eval()\n    test_loss = 0\n\n    #set up so we're not calculating gradients\n    with torch.no_grad():\n        for X, y in dataloader:\n            X, y = X.to(device), y.to(device)\n            pred = model(X)\n            test_loss += loss_fn(pred, y).item() * X.size(0)\n    avg_loss = test_loss / size\n    print(f\"Avg Loss: {avg_loss:>7f}\\n\")\n\n```\n:::\n\n\nAnd we can walk through it:\n\n- In this function, `size` is actually useful in calculating our loss. We have to do some kinda wonky slight-of-hand when estimating the model loss here. We are using mean-squared error (MSE) as our loss function, and for each batch in the dataloader, it will give us the mean-squared error (a single number per batch). We then multiply this average loss by the size of the batch to get the \"total\" loss per batch, and we sum up all of the total batch losses to get the total overall loss (`test_loss` in the function). Since this is the total loss, we then have to divide by the number of observations to get us back to the mean squared error.\n- `model.eval()` puts the model into evaluation mode, signaling that we're not going to be calculating gradients or anything like that.\n- We initialize `test_loss` to 0\n- Then for the remainder, we make predictions (just like we did in the `train()` function) and calculate loss as described in the first bullet point above.\n\n### Train the Model\n\nNow we can finally train the model. We'll train over 5 \"epochs\", i.e. 5 passes through the full dataset. This is arbitrary here -- in real-world contexts this is a number we probably want to tune for or at least choose carefully.\n\nWe do the traning with a simple `for` loop, and during each iteration through the loop we:\n\n1. train the model;\n2. show the performance on the test set\n\nSince we included some print statements in our `train()` and `test()` functions, we can monitor the progress of our model's training.\n\n::: {#b5b10aa7 .cell execution_count=8}\n``` {.python .cell-code}\nepochs = 5\nfor i in range(epochs):\n    print(f\"Epoch {i+1}\\n------------------\")\n    train(trn_dl, model, loss_fn, opt)\n    test(tst_dl, model, loss_fn)\nprint(\"Done!\")\n\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 1\n------------------\nloss: 5.130811 [   64/ 8000]\nloss: 7.558138 [  704/ 8000]\nloss: 6.512156 [ 1344/ 8000]\nloss: 5.550046 [ 1984/ 8000]\nloss: 7.068350 [ 2624/ 8000]\nloss: 6.254210 [ 3264/ 8000]\nloss: 6.333334 [ 3904/ 8000]\nloss: 6.672968 [ 4544/ 8000]\nloss: 6.011800 [ 5184/ 8000]\nloss: 6.281922 [ 5824/ 8000]\nloss: 5.252160 [ 6464/ 8000]\nloss: 5.972773 [ 7104/ 8000]\nloss: 5.569077 [ 7744/ 8000]\nAvg Loss: 5.857892\n\nEpoch 2\n------------------\nloss: 4.593849 [   64/ 8000]\nloss: 6.697829 [  704/ 8000]\nloss: 5.681279 [ 1344/ 8000]\nloss: 4.791214 [ 1984/ 8000]\nloss: 5.994133 [ 2624/ 8000]\nloss: 5.362798 [ 3264/ 8000]\nloss: 5.297074 [ 3904/ 8000]\nloss: 5.547976 [ 4544/ 8000]\nloss: 4.942132 [ 5184/ 8000]\nloss: 4.995830 [ 5824/ 8000]\nloss: 4.198215 [ 6464/ 8000]\nloss: 4.676771 [ 7104/ 8000]\nloss: 4.366837 [ 7744/ 8000]\nAvg Loss: 4.493579\n\nEpoch 3\n------------------\nloss: 3.628562 [   64/ 8000]\nloss: 5.131055 [  704/ 8000]\nloss: 4.146622 [ 1344/ 8000]\nloss: 3.396595 [ 1984/ 8000]\nloss: 4.028492 [ 2624/ 8000]\nloss: 3.745448 [ 3264/ 8000]\nloss: 3.482335 [ 3904/ 8000]\nloss: 3.576678 [ 4544/ 8000]\nloss: 3.107245 [ 5184/ 8000]\nloss: 2.927510 [ 5824/ 8000]\nloss: 2.533787 [ 6464/ 8000]\nloss: 2.630936 [ 7104/ 8000]\nloss: 2.594630 [ 7744/ 8000]\nAvg Loss: 2.529012\n\nEpoch 4\n------------------\nloss: 2.220653 [   64/ 8000]\nloss: 2.848730 [  704/ 8000]\nloss: 2.130622 [ 1344/ 8000]\nloss: 1.705244 [ 1984/ 8000]\nloss: 1.762509 [ 2624/ 8000]\nloss: 1.957887 [ 3264/ 8000]\nloss: 1.728549 [ 3904/ 8000]\nloss: 1.747250 [ 4544/ 8000]\nloss: 1.522444 [ 5184/ 8000]\nloss: 1.378201 [ 5824/ 8000]\nloss: 1.367811 [ 6464/ 8000]\nloss: 1.194838 [ 7104/ 8000]\nloss: 1.488245 [ 7744/ 8000]\nAvg Loss: 1.383161\n\nEpoch 5\n------------------\nloss: 1.386802 [   64/ 8000]\nloss: 1.463986 [  704/ 8000]\nloss: 1.139213 [ 1344/ 8000]\nloss: 0.983471 [ 1984/ 8000]\nloss: 0.841175 [ 2624/ 8000]\nloss: 1.235995 [ 3264/ 8000]\nloss: 1.212063 [ 3904/ 8000]\nloss: 1.196411 [ 4544/ 8000]\nloss: 1.063424 [ 5184/ 8000]\nloss: 1.051633 [ 5824/ 8000]\nloss: 1.127206 [ 6464/ 8000]\nloss: 0.833066 [ 7104/ 8000]\nloss: 1.255609 [ 7744/ 8000]\nAvg Loss: 1.170692\n\nDone!\n```\n:::\n:::\n\n\nAnd that's a complete step-by-step for a fully-connected neural net!\n\n",
    "supporting": [
      "pytorch_fully_connected_files"
    ],
    "filters": [],
    "includes": {}
  }
}
[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "Hi, friend. I’m Eric. If you’re interested in learning more about me, you can check out my personal site, where I occassionally blog and post stuff. You can also find me on Github"
  },
  {
    "objectID": "Go/interface_composition.html",
    "href": "Go/interface_composition.html",
    "title": "Interface Composition",
    "section": "",
    "text": "One cool feature of Go is that it lets you compose interfaces using other interfaces. The prototypical example of this is the ReadWriter, which is composed of the Reader interface and the Writer interface.\nBelow is an example of a toy program that defines a UserStorer interface and an ItemStorer interface, then a Storer interface that is composed of these interfaces"
  },
  {
    "objectID": "Go/interface_composition.html#define-user-stuff",
    "href": "Go/interface_composition.html#define-user-stuff",
    "title": "Interface Composition",
    "section": "Define User Stuff",
    "text": "Define User Stuff\npackage main\n\nimport (\n    \"encoding/json\"\n    \"fmt\"\n)\n\n//create type\ntype User struct {\n    Name string\n    Age  int\n}\n\n//create interface\ntype UserStorer interface {\n    AddUser(users []*User, user *User) []*User\n}\n\n//define an empty struct that will implement interface\ntype ustore struct{}\n\n//constructor\nfunc NewUserStore() UserStorer {\n    return &ustore{}\n}\n\nfunc (us ustore) AddUser(users []*User, user *User) []*User {\n\n    users = append(users, user)\n\n    return users\n}"
  },
  {
    "objectID": "Go/interface_composition.html#define-item-stuff",
    "href": "Go/interface_composition.html#define-item-stuff",
    "title": "Interface Composition",
    "section": "Define Item Stuff",
    "text": "Define Item Stuff\n//define type\ntype Item struct {\n    Name  string\n    Usage string\n}\n\n//define interface\ntype ItemStorer interface {\n    AddItem(items []*Item, item *Item) []*Item\n}\n\n//define empty struct that will implement interface\ntype istore struct{}\n\n//define constructor\nfunc NewItemStore() ItemStorer {\n    return &istore{}\n}\n\n//define add method\nfunc (is istore) AddItem(items []*Item, item *Item) []*Item {\n\n    items = append(items, item)\n\n    return items\n}"
  },
  {
    "objectID": "Go/interface_composition.html#define-storer-stuff",
    "href": "Go/interface_composition.html#define-storer-stuff",
    "title": "Interface Composition",
    "section": "Define Storer Stuff",
    "text": "Define Storer Stuff\ntype Storer interface {\n    UserStorer\n    ItemStorer\n}\n\ntype myStore struct {\n    UserStorer\n    ItemStorer\n}\n\nfunc NewStorer(us UserStorer, is ItemStorer) Storer {\n\n    s := &myStore{\n        UserStorer: us,\n        ItemStorer: is,\n    }\n\n    return s\n}"
  },
  {
    "objectID": "Go/interface_composition.html#create-simple-program",
    "href": "Go/interface_composition.html#create-simple-program",
    "title": "Interface Composition",
    "section": "Create Simple Program",
    "text": "Create Simple Program\nfunc main() {\n\n    us := NewUserStore()\n    is := NewItemStore()\n\n    s := NewStorer(us, is)\n\n    var items []*Item\n    var users []*User\n\n    u := &User{\n        Name: \"Eric\",\n        Age:  34,\n    }\n\n    i := &Item{\n        Name:  \"Phone\",\n        Usage: \"texting\",\n    }\n\n    items = s.AddItem(items, i)\n    users = s.AddUser(users, u)\n\n    //embed to json\n    ji, _ := json.Marshal(items)\n    ju, _ := json.Marshal(users)\n\n    //print out results\n    fmt.Println(string(ji))\n    fmt.Println(string(ju))\n}"
  },
  {
    "objectID": "Go/parse_query_params.html",
    "href": "Go/parse_query_params.html",
    "title": "Parsing URL Query Parameters",
    "section": "",
    "text": "This is a small example of how to parse url query parameters in Go. I was learning this in service of another project, in which case I wanted the parameters in the map[string]string format, but this doesn’t always need to be the case.\npackage main\n\nimport (\n    \"encoding/json\"\n    \"fmt\"\n    \"log\"\n    \"net/http\"\n    \"strings\"\n)\n\nfunc DemoGet(w http.ResponseWriter, r *http.Request) {\n\n    urlParams := r.URL.Query()\n\n    //in a use case for another project, i want the urlParams to be encoded as a map[string]string\n    //but this won't always be the use case\n    m := make(map[string]string, len(urlParams))\n\n    for i, v := range urlParams {\n        //in prod, we probably don't want to error out\n        //but yolo for now\n        if len(v) &gt; 1 {\n            log.Fatal(\"query parameters should all be length 1\")\n        }\n        s := strings.Join(v, \"\")\n        m[i] = s\n    }\n\n    w.Header().Set(\"Content-Type\", \"application/json\")\n\n    json.NewEncoder(w).Encode(m)\n}\n\nfunc main() {\n    r := http.NewServeMux()\n\n    r.HandleFunc(\"/demoget\", DemoGet)\n\n    s := &http.Server{\n        Addr:    \":8080\",\n        Handler: r,\n    }\n\n    fmt.Println(\"Running demo program\")\n\n    s.ListenAndServe()\n}"
  },
  {
    "objectID": "Go/post_request_validation.html",
    "href": "Go/post_request_validation.html",
    "title": "Validating Post Requests",
    "section": "",
    "text": "This is a pretty small example of how to validate a post request in Go. It’s not at all difficult, but I’m writing this because I often forget to do this, and I’m hoping that writing it will make me remember.\npackage main\n\nimport (\n    \"encoding/json\"\n    \"fmt\"\n    \"log\"\n    \"net/http\"\n\n    \"github.com/go-playground/validator\"\n)\n\nconst (\n    listenAddr string = \":8080\"\n)\n\n//when defining the struct, be sure to indicate validation requirements\n//you can also do something like validate:\"required,email\" to check that the input is an email\ntype User struct {\n    FirstName string `json:\"firstName\" validate:\"required\"`\n    LastName  string `json:\"lastName\" validate:\"required\"`\n}\n\nfunc handleIndex(w http.ResponseWriter, r *http.Request) {\n\n    switch r.Method {\n    case http.MethodGet:\n        msg := \"hello, world\"\n\n        w.WriteHeader(http.StatusOK)\n\n        w.Write([]byte(msg))\n\n    case http.MethodPost:\n        var u *User\n\n        //create a new instance of a validator\n        validate := validator.New()\n\n        json.NewDecoder(r.Body).Decode(&u)\n\n        //validate the struct\n        err := validate.Struct(u)\n\n        if err != nil {\n            http.Error(w, \"invalid request\", http.StatusBadRequest)\n            return\n        }\n\n        msg := \"hello, \" + u.FirstName + \" \" + u.LastName\n\n        w.WriteHeader(http.StatusOK)\n\n        w.Write([]byte(msg))\n    default:\n        http.Error(w, \"method not allowed\", http.StatusMethodNotAllowed)\n    }\n}\n\nfunc main() {\n    r := http.NewServeMux()\n\n    r.HandleFunc(\"/\", handleIndex)\n\n    fmt.Println(\"Running server\")\n\n    err := http.ListenAndServe(listenAddr, r)\n\n    log.Fatal(err)\n}"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "leakybrain",
    "section": "",
    "text": "Welcome to leakybrain.\nThis is a site I created to drop random snippets of code and take notes related to programming, statistics, and other related topics. It was inspired by Vicki Boykis’s BoringML project."
  },
  {
    "objectID": "Julia/julia_pkg_init.html",
    "href": "Julia/julia_pkg_init.html",
    "title": "Julia Package Initialization",
    "section": "",
    "text": "Creating a new Julia package is easiest if you create whatever.jl script to help establish the template, e.g.\ncd some/path\ntouch whatever.jl\nThen open this with VSCode. Once in VSCode, open the terminal and install PkgTemplates.jl if it’s not already installed:\n]\nadd PkgTemplates\nThen create a template with the options you want in your new package. The way I currently do this is:\nusing PkgTemplates\n\nt = Template(;\n    user = \"ekholme\", #or your git user.name\n    license = \"MIT\",\n    authors = [\"Eric Ekholm\"],\n    julia_version = v\"1.8\", #or w/e version you want\n    plugins = [\n        GitHubActions(),\n        Codecov(),\n        GitHubPages(),\n        TravisCI()\n    ]\n)\n\nt(\"MyPkgName\")\nAnd then running this will create a new package (and set up all of your Git stuff if it’s already configured)"
  },
  {
    "objectID": "Julia/mlj_pipeline.html",
    "href": "Julia/mlj_pipeline.html",
    "title": "MLJ Pipeline",
    "section": "",
    "text": "Below is a minimal (yet complete) example of a machine learning pipeline that use’s Julia’s MLJ framework and the Palmer Penguins dataset.\nNote that the goal here isn’t necessarily to fit the best model; rather it’s just to demonstrate an MLJ pipeline.\n\nusing DataFrames\nusing CSV\nusing Random\nusing MLJ\n\nRandom.seed!(0408)\n\n#get penguins data\npenguins = CSV.read(download(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-07-28/penguins.csv\"), DataFrame, missingstring=\"NA\")\n\n#filter to those without missing body mass\ndropmissing!(penguins, :body_mass_g)\n\n#extract body mass as y\ny, X = unpack(penguins, ==(:body_mass_g))\n\n# coercing textual columns to multiclass for modeling\ncoerce_nms = [:species, :sex, :island]\n\nc_dict = Dict(zip(coerce_nms, repeat([Multiclass], 3)))\n\ncoerce!(\n    X,\n    c_dict\n)\n\n#get training and validation indices\ntrn, val = partition(eachindex(y), 0.8; shuffle=true)\n\n#define pipeline components\nimp = FillImputer();\nstand = Standardizer();\noh = OneHotEncoder(drop_last=true);\nLinearRegression = @load LinearRegressor pkg = GLM add = true\nmod = LinearRegression()\n\n#define pipeline\nm = Pipeline(imp, stand, oh, mod)\n\n#define machine\nmach = machine(m, X, y);\n\n#fit machine on training rows\nfit!(mach, rows=trn)\n\n#predicting training y's\nŷ = MLJ.predict_mean(mach, X[trn, :])\n\n#evaluate model\ncv = CV(nfolds=3)\n\nMLJ.evaluate!(mach, rows=val, resampling=cv, measure=rmse)\n\n#note -- call measures() to see all available measures\n\n┌ Info: Trying to coerce from `Union{Missing, String7}` to `Multiclass`.\n└ Coerced to `Union{Missing,Multiclass}` instead.\n[ Info: For silent loading, specify `verbosity=0`. \n[ Info: Training machine(ProbabilisticPipeline(fill_imputer = FillImputer(features = Symbol[], …), …), …).\n[ Info: Training machine(:fill_imputer, …).\n[ Info: Training machine(:standardizer, …).\n[ Info: Training machine(:one_hot_encoder, …).\n[ Info: Spawning 2 sub-features to one-hot encode feature :species.\n[ Info: Spawning 2 sub-features to one-hot encode feature :island.\n[ Info: Spawning 1 sub-features to one-hot encode feature :sex.\n[ Info: Training machine(:linear_regressor, …).\n[ Info: Creating subsamples from a subset of all rows. \nEvaluating over 3 folds:  67%[================&gt;        ]  ETA: 0:00:01Evaluating over 3 folds: 100%[=========================] Time: 0:00:02\n\n\nimport MLJGLMInterface ✔\n\n\n\nPerformanceEvaluation object with these fields:\n  measure, operation, measurement, per_fold,\n  per_observation, fitted_params_per_fold,\n  report_per_fold, train_test_rows\nExtract:\n┌────────────────────────┬──────────────┬─────────────┬─────────┬───────────────\n│ measure                │ operation    │ measurement │ 1.96*SE │ per_fold     ⋯\n├────────────────────────┼──────────────┼─────────────┼─────────┼───────────────\n│ RootMeanSquaredError() │ predict_mean │ 342.0       │ 94.8    │ [277.0, 322. ⋯\n└────────────────────────┴──────────────┴─────────────┴─────────┴───────────────\n                                                                1 column omitted"
  },
  {
    "objectID": "Python/sklearn_pipeline.html",
    "href": "Python/sklearn_pipeline.html",
    "title": "Scikit-Learn Pipeline",
    "section": "",
    "text": "Below is a minimal (yet complete) example of a machine learning pipeline using python and scikit-learn and the Palmer Penguins dataset.\nNote that the goal here isn’t necessarily to fit the best model; rather it’s just to demonstrate an sklearn pipeline. Also note that I wouldn’t call myself an expert python programmer, so there may be better/more efficient ways to do this.\n\nimport polars as pl\nimport numpy as np\nimport math\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.metrics import mean_squared_error\n\npenguins = pl.read_csv(\n    \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-07-28/penguins.csv\",\n    null_values=\"NA\",\n)\n\n# filtering to only rows with available body mass data\npenguins_complete = penguins.filter(pl.col(\"body_mass_g\").is_not_null())\n\n# coercing null to nan\npenguins_complete = penguins_complete.with_columns(pl.all().fill_null(np.nan))\n\n# separate into X and y\ny = penguins_complete[\"body_mass_g\"]\nX = penguins_complete.select(pl.exclude(\"body_mass_g\"))\n\n# coerce X to pandas since polars dfs don't seem to be supported for all of the sklearn steps yet\nX = X.to_pandas()\n\n# train test split\nX_trn, X_tst, y_trn, y_tst = train_test_split(X, y, random_state=408)\n\n# create pipeline for categorical features\ncat_feats = [\"species\", \"island\", \"sex\"]\ncat_transform = Pipeline(\n    [\n        (\"cat_imputer\", SimpleImputer(strategy=\"most_frequent\")),\n        (\"oh_encoder\", OneHotEncoder(drop=\"first\")),\n    ]\n)\n\n# create pipeline for numerical features\ncont_feats = [\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"year\"]\ncont_transform = Pipeline(\n    [\n        (\"cont_imputer\", SimpleImputer(strategy=\"mean\")),\n        (\"standardizer\", StandardScaler()),\n    ]\n)\n\n# create a preprocessing pipeline\npreprocessor = ColumnTransformer(\n    [(\"cats\", cat_transform, cat_feats), (\"conts\", cont_transform, cont_feats)]\n)\n\n# make full pipeline\npipe = Pipeline([(\"preprocess\", preprocessor), (\"lin_reg\", LinearRegression())])\n\n# fit pipeline\npipe.fit(X_trn, y_trn)\n\n# predict training y's\ny_hat = pipe.predict(X_trn)\n\n# evaluate model\ny_hat_tst = pipe.predict(X_tst)\n\nmath.sqrt(mean_squared_error(y_tst, y_hat_tst))\n\n295.1333053249438"
  },
  {
    "objectID": "Python/venv.html",
    "href": "Python/venv.html",
    "title": "venvs in Python",
    "section": "",
    "text": "To create a venv:\npython3.11 -m venv ./venv\nand to activate it:\nsource venv/bin/activate\net voila"
  },
  {
    "objectID": "R/pre_alloc_loops.html",
    "href": "R/pre_alloc_loops.html",
    "title": "Pre-Allocating to Improve For Loops",
    "section": "",
    "text": "How to pre-allocate vectors used in your for loops in R to make your code run faster and perform fewer allocations.\nFirst, set n to be the size of the vector we want to work with\n\nn &lt;- 1e6\n\nThen set up a function that doesn’t pre-allocate a list\n\nno_alloc &lt;- function(n) {\n\n    x &lt;- list()\n\n    for (i in seq_len(n)) {\n        x[[i]] &lt;- i\n    }\n\n    x\n}\n\nThen set up a function that does pre-allocate a list\n\npre_alloc &lt;- function(n) {\n    \n    x &lt;- vector(mode = \"list\", length = n)\n\n    for (i in seq_len(n)) {\n        x[[i]] &lt;- i\n    }\n    \n    x\n}\n\nAnd compare the benchmarks for these functions\n\nlibrary(bench)\n\nres &lt;- bench::mark(\n    no_alloc(n),\n    pre_alloc(n)\n)\n\nWarning: Some expressions had a GC in every iteration; so filtering is disabled.\n\nres[c(\"expression\", \"min\", \"median\", \"itr/sec\", \"n_gc\")]\n\n# A tibble: 2 × 4\n  expression        min   median `itr/sec`\n  &lt;bch:expr&gt;   &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt;\n1 no_alloc(n)   660.4ms  660.4ms      1.51\n2 pre_alloc(n)   60.6ms   64.6ms     12.6 \n\n\nWe can see that pre-allocating is the way to go!"
  },
  {
    "objectID": "R/s3.html",
    "href": "R/s3.html",
    "title": "S3 Methods",
    "section": "",
    "text": "Below is a sort of minimal example of S3 methods, including how to define them for various classes.\n\nDefine a Generic\n\ncombine &lt;- function(x) {\n    UseMethod(\"combine\")\n} \n\n\n\nDefine and Instantiate Some Classes\n\n# constructor for a new instance of 'my_class'\nnew_my_class &lt;- function(x, y) {\n    stopifnot(is.character(x) & is.character(y))\n\n    structure(\n        list(x = x, y = y),\n        class = \"my_class\"\n    )\n} \n\n# constructor for a new instance of 'your_class'\nnew_your_class &lt;- function(x, y) {\n    stopifnot(is.numeric(x) & is.numeric(y))\n\n    structure(\n        list(x = x, y = y),\n        class = \"your_class\"\n    )\n}\n\na &lt;- new_my_class(\"aaa\", \"bbb\")\nb &lt;- new_your_class(1, 2)\n\n\n\nDefine Combine Methods for Each Class\n\ncombine.my_class &lt;- function(x) {\n    paste0(\n        vctrs::field(x, \"x\"),\n        vctrs::field(x, \"y\")\n    )\n} \n\ncombine.your_class &lt;- function(x) {\n    a &lt;- vctrs::field(x, \"x\")\n    b &lt;- vctrs::field(x, \"y\")\n\n    a + b\n}\n\n\n\nCall Methods\n\ncombine(a) \n\n[1] \"aaabbb\"\n\n\n\ncombine(b) \n\n[1] 3\n\n\nhuzzah!"
  },
  {
    "objectID": "R/tidymodels_workflow.html",
    "href": "R/tidymodels_workflow.html",
    "title": "Tidymodels Workflow",
    "section": "",
    "text": "Below is a minimal (yet complete) example of a machine learning pipeline that use’s R’s tidymodels framework and the Palmer Penguins dataset.\nNote that the goal here isn’t necessarily to fit the best model or demonstrate all of the features; rather it’s just to demonstrate a tidymodels workflow.\n\nlibrary(tidymodels)\nlibrary(tidyverse)\n\nset.seed(0408)\n\n# there's a package for this, but let's just grab the csv\npenguins &lt;- read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-07-28/penguins.csv\")\n\n# drop rows missing body mass\npenguins_complete &lt;- penguins |&gt;\n    filter(!is.na(body_mass_g))\n\n# split the data into training and validation\npenguins_split &lt;- initial_split(penguins, prop = .8)\n\ntrn &lt;- training(penguins_split)\nval &lt;- testing(penguins_split)\n\n# define a recipe for preprocessing\npenguins_rec &lt;- recipe(body_mass_g ~ ., data = trn) |&gt;\n    step_impute_mode(all_nominal_predictors()) |&gt;\n    step_impute_mean(all_numeric_predictors()) |&gt;\n    step_normalize(all_numeric_predictors()) |&gt;\n    step_dummy(all_nominal_predictors())\n\n# define a model specification\nlm_spec &lt;- linear_reg() |&gt;\n    set_engine(\"lm\")\n\n# define a workflow with our preprocessor and our model\nwf &lt;- workflow(penguins_rec, lm_spec)\n\n# fit the workflow\nwf_fit &lt;- wf |&gt;\n    fit(data = trn)\n\n# predict testing data\ny_hat &lt;- unlist(predict(wf_fit, new_data = val))\n\n# estimate performance\neval_tbl &lt;- tibble(\n    truth = val$body_mass_g,\n    estimate = y_hat\n)\n\nrmse(eval_tbl, truth, estimate)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard        315."
  },
  {
    "objectID": "Stats/gradient_descent.html",
    "href": "Stats/gradient_descent.html",
    "title": "Gradient Descent",
    "section": "",
    "text": "An example of estimating linear regression beta coefficients via gradient descent, using Julia.\n\nusing Random\nusing ForwardDiff\nusing Distributions\nusing Statistics\n\n\nGenerate Data\nFirst, we generate some fake data\n\nRandom.seed!(0408)\n\n#x data\n𝐗 = hcat(ones(1000), randn(1000, 3))\n\n#ground truth betas\n𝚩 = [.5, 1, 2, 3]\n\n#multiply data by betas\nf₁(X) = X*𝚩\n\n#make some error\nϵ = rand(Normal(0, .5), size(𝐗)[1])\n\n#generate y\ny = f₁(𝐗) + ϵ;\n\n\n\nDefine a Loss Function\nMean squared error is the most straightforward\n\nfunction mse_loss(X, y, b)\n    ŷ = X*b\n\n    l = mean((y .- ŷ).^2)\n\n    return l\nend\n\nmse_loss (generic function with 1 method)\n\n\n\n\nDefine a training function\nThis implements the gradient descent algorithm:\n\ninitialize some random beta values\ninitialize error as some very large number (the init value doesn’t really matter as long as it’s greater than the function’s tol parameter)\ninitialize the number of iterations (iter) at 0\ndefine a function d() to get the gradient of the loss function at a given set of betas\ndefine a loop that updates the beta values by the learning rate * the gradients until convergence\n\n\nfunction grad_descent(X, y; lr = .01, tol = .01, max_iter = 1_000, noisy = false)\n   #randomly initialize betas\n   β = rand(size(X)[2])\n   \n    #init error to something large\n    err = 1e10\n\n    #initialize iterations at 0\n    iter = 0\n\n    #define a function to get the gradient of the loss function at a given set of betas\n    d(b) = ForwardDiff.gradient(params -&gt; mse_loss(X, y, params), b)\n\n    while err &gt; tol && iter &lt; max_iter\n        β -= lr*d(β)\n        err = mse_loss(X, y, β)\n        if (noisy == true)\n            println(\"Iteration $(iter): current error is $(err)\")\n        end\n        iter += 1\n    end\n    return β\nend\n\ngrad_descent (generic function with 1 method)\n\n\n\n\nEstimate βs\nTo estimate the betas, we just run the function\n\nb = grad_descent(𝐗, y)\n\n4-element Vector{Float64}:\n 0.5220524143318362\n 0.992503536801155\n 1.9951668587882012\n 2.997961983119764\n\n\n\n\nCheck Solution Against Base Julia Solver\n\n𝐗 \\ y .≈ b\n\n4-element BitVector:\n 1\n 1\n 1\n 1\n\n\nhuzzah!"
  },
  {
    "objectID": "Stats/mle_lm.html",
    "href": "Stats/mle_lm.html",
    "title": "Maximum Likelihood Estimation - Linear Regression",
    "section": "",
    "text": "An example of estimating regression coefficients in a linear model via maximum likelihood, using Julia.\n\nusing Distributions\nusing Random\nusing Optim\nusing GLM\n\n┌ Info: Precompiling Optim [429524aa-4258-5aef-a3af-852621145aeb]\n└ @ Base loading.jl:1664\n\n\n┌ Info: Precompiling GLM [38e38edf-8417-5370-95a0-9cbb8c7f171a]\n└ @ Base loading.jl:1664\n\n\nGenerate some fake data\n\nRandom.seed!(0408)\n\n#x data\n𝐗 = hcat(ones(1000), randn(1000, 3))\n\n#ground truth betas\n𝚩 = [.5, 1, 2, 3]\n\n#multiply data by betas\nf₁(X) = X*𝚩\n\n#make some error\nϵ = rand(Normal(0, .5), size(𝐗)[1])\n\n#generate y\ny = f₁(𝐗) + ϵ;\n\nDefine a function to optimize\n\nfunction mle_lm(x, y, params)\n    b = params[begin:end-1]\n    σ = params[end]\n\n    ŷ = x*b\n\n    residuals = y .- ŷ\n\n    ll = -loglikelihood(Normal(0, σ), residuals)\n\n    return ll\nend\n\nmle_lm (generic function with 1 method)\n\n\nRun the optimization\n\nstart_params = [0.2, .5, 1, 1, 1]\n\nres = optimize(params -&gt; mle_lm(𝐗, y, params), start_params)\n\nOptim.minimizer(res)\n\n5-element Vector{Float64}:\n 0.5220526008168841\n 0.9925044101015756\n 1.9951661029827337\n 2.9979617225853903\n 0.5170359122024899\n\n\nCheck against ‘base’ Julia solution\n\n𝐗 \\ y\n\n4-element Vector{Float64}:\n 0.5220524130050493\n 0.9925035386795751\n 1.9951668631756507\n 2.9979619869409357"
  },
  {
    "objectID": "Stats/turing_lm.html",
    "href": "Stats/turing_lm.html",
    "title": "Bayesian Linear Regression",
    "section": "",
    "text": "An example of using Bayesian methods (via Julia’s Turing.jl) to estimate a linear regression\nLoad Packages\n\nusing Turing\nusing Random\nusing Distributions\nusing LinearAlgebra\nusing Plots\nusing StatsPlots\n\nGenerate some fake data\n\nRandom.seed!(0408)\n\nn = 1000\n\n𝐗 = randn(n, 3)\n\nβ = [1., 2., 3.]\n\nf(x) = .5 .+ x*β\n\nϵ = rand(Normal(0, .2), n)\n\ny = f(𝐗) + ϵ;\n\nDefine a Model\n\n@model function linear_regression(x, y)\n    #housekeeping\n    n_feat = size(x, 2)\n    \n    #priors\n    α ~ Normal(0, 2)\n    σ ~ Exponential(1)\n    b ~ MvNormal(zeros(n_feat), 5 * I)\n\n    #likelihood\n    for i ∈ eachindex(y)\n        y[i] ~ Normal(α + x[i,:]' * b, σ)\n    end\nend\n\nlinear_regression (generic function with 2 methods)\n\n\nCompute Posterior\n\nmodel = linear_regression(𝐗, y)\n\nchn = sample(model, NUTS(), MCMCThreads(), 1_000, 2);\n\nPlot Parameter Posteriors\n\nplot(chn)\n\n\n\n\nPredict Values of Y\n\npred_mod = linear_regression(\n    𝐗, \n    Vector{Union{Missing, Float64}}(undef, length(y))\n)\n\npreds = predict(pred_mod, chn);\n\n#to get summary statistics\nsummarize(preds)\n\n\nSummary Statistics\n  parameters      mean       std   naive_se      mcse         ess      rhat \n      Symbol   Float64   Float64    Float64   Float64     Float64   Float64 \n        y[1]    5.6876    0.2120     0.0047    0.0040   2131.4185    1.0007\n        y[2]   -1.0132    0.2088     0.0047    0.0050   2045.6712    1.0004\n        y[3]   -3.1168    0.2059     0.0046    0.0039   2026.1454    0.9993\n        y[4]    4.5125    0.2042     0.0046    0.0047   1994.3306    0.9997\n        y[5]    2.6687    0.2041     0.0046    0.0036   2064.6256    0.9991\n        y[6]   -1.2255    0.2080     0.0047    0.0049   1836.3394    0.9993\n        y[7]    2.3067    0.2054     0.0046    0.0039   2075.1743    1.0003\n        y[8]    1.1287    0.2042     0.0046    0.0039   1929.8026    0.9999\n        y[9]    0.4400    0.2061     0.0046    0.0041   2109.2564    0.9996\n       y[10]    6.7965    0.2053     0.0046    0.0047   1746.0770    1.0008\n       y[11]   -3.7930    0.2034     0.0045    0.0048   1820.9121    0.9993\n       y[12]    0.4814    0.2053     0.0046    0.0046   1868.2305    0.9996\n       y[13]    2.4322    0.2068     0.0046    0.0045   1941.8888    0.9993\n       y[14]    0.3437    0.2071     0.0046    0.0048   1761.8573    1.0017\n       y[15]   -0.7060    0.2091     0.0047    0.0046   2053.6154    1.0004\n       y[16]    0.3729    0.2086     0.0047    0.0048   1927.2731    0.9993\n       y[17]    1.8546    0.2099     0.0047    0.0047   1810.0899    0.9996\n       y[18]    2.2985    0.2073     0.0046    0.0045   1978.9502    0.9994\n       y[19]   -0.8137    0.2073     0.0046    0.0047   2090.7345    0.9995\n       y[20]    4.8043    0.2024     0.0045    0.0048   1878.8739    0.9993\n       y[21]    3.7083    0.2100     0.0047    0.0052   1885.7547    0.9996\n       y[22]    0.2769    0.2095     0.0047    0.0036   1944.5012    0.9996\n       y[23]   -3.4487    0.2122     0.0047    0.0054   2100.7819    1.0001\n      ⋮           ⋮         ⋮         ⋮          ⋮          ⋮          ⋮\n                                                             977 rows omitted\n\n\n\n\nPlot posterior distribution(s) of the predictions for the first observation:\n\ny_1 = getindex(preds, \"y[1]\")\n\ndensity(y_1.data)\n\n\n\n\nAnd to get mean predicted values for each observation of y:\n\nmean_preds = summarize(preds)[:, 2]\n\n1000-element Vector{Float64}:\n  5.687584896870903\n -1.0131932466821032\n -3.116780621735853\n  4.512509987465371\n  2.6687246425133946\n -1.2254837248151988\n  2.306673192525782\n  1.1286618139435005\n  0.44000862835633064\n  6.79652703562579\n -3.79298730457439\n  0.48144060256761656\n  2.4321882922004816\n  ⋮\n -3.662775430736099\n  0.7859815782511625\n -3.5565609003804592\n -0.9696845244200145\n  0.82043625177212\n  4.348165410409958\n  1.9125271244452688\n  1.072526007780797\n -7.133862685203614\n -6.206271121676054\n -7.631370737446044\n  0.7668219857089003"
  }
]
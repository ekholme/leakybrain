{
  "hash": "2088249100175e157580fe66f6b61d17",
  "result": {
    "markdown": "---\ntitle: Gradient Descent\ndescription: |\n  Illustrated using Julia\ndate: '2023-04-21'\nformat:\n  html:\n    code-fold: false\n---\n\nAn example of estimating linear regression beta coefficients via gradient descent, using Julia.\n\n::: {.cell execution_count=1}\n``` {.julia .cell-code}\nusing Random\nusing GLM\nusing ForwardDiff\nusing Statistics\n```\n:::\n\n\n# Generate Data\n\nFirst, we generate some fake data\n\n::: {.cell execution_count=2}\n``` {.julia .cell-code}\nRandom.seed!(0408)\n\n#x data\nğ— = hcat(ones(1000), randn(1000, 3))\n\n#ground truth betas\nğš© = [.5, 1, 2, 3]\n\n#multiply data by betas\nfâ‚(X) = X*ğš©\n\n#make some error\nÏµ = rand(Normal(0, .5), size(ğ—)[1])\n\n#generate y\ny = fâ‚(ğ—) + Ïµ;\n```\n:::\n\n\n# Define a Loss Function\n\nMean squared error is the most straightforward\n\n::: {.cell execution_count=3}\n``` {.julia .cell-code}\nfunction mse_loss(X, y, b)\n    yÌ‚ = X*b\n\n    l = mean((y .- yÌ‚).^2)\n\n    return l\nend\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```\nmse_loss (generic function with 1 method)\n```\n:::\n:::\n\n\n# Define a training function\n\nThis implements the gradient descent algorithm:\n\n- initialize some random beta values\n- initialize error as some very large number (the init value doesn't really matter as long as it's greater than the function's `tol` parameter)\n- initialize the number of iterations (`iter`) at 0\n- define a function `d()` to get the gradient of the loss function at a given set of betas\n- define a loop that updates the beta values by the learning rate * the gradients until convergence\n\n::: {.cell execution_count=4}\n``` {.julia .cell-code}\nfunction grad_descent(X, y; lr = .01, tol = .01, max_iter = 1_000, noisy = false)\n   #randomly initialize betas\n   Î² = rand(size(X)[2])\n   \n    #init error to something large\n    err = 1e10\n\n    #initialize iterations at 0\n    iter = 0\n\n    #define a function to get the gradient of the loss function at a given set of betas\n    d(b) = ForwardDiff.gradient(params -> mse_loss(X, y, params), b)\n\n    while err > tol && iter < max_iter\n        Î² -= lr*d(Î²)\n        err = mse_loss(X, y, Î²)\n        if (noisy == true)\n            println(\"Iteration $(iter): current error is $(err)\")\n        end\n        iter += 1\n    end\n    return Î²\nend\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\ngrad_descent (generic function with 1 method)\n```\n:::\n:::\n\n\n# Estimate Î²s\n\nTo estimate the betas, we just run the function\n\n::: {.cell execution_count=5}\n``` {.julia .cell-code}\nb = grad_descent(ğ—, y)\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```\n4-element Vector{Float64}:\n 0.5220524143318362\n 0.992503536801155\n 1.9951668587882012\n 2.997961983119764\n```\n:::\n:::\n\n\n# Check Solution Against Base Julia Solver\n\n::: {.cell execution_count=6}\n``` {.julia .cell-code}\nğ— \\ y .â‰ˆ b\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```\n4-element BitVector:\n 1\n 1\n 1\n 1\n```\n:::\n:::\n\n\nhuzzah!\n\n",
    "supporting": [
      "gradient_descent_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}